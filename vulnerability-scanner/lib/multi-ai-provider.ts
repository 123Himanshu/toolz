/**
 * Multi AI Provider
 * Supports: Gemini, Groq
 */

interface AIResponse {
  content: string;
  provider: 'gemini' | 'groq';
}

/**
 * Get AI completion from available provider
 * Priority: Groq > Gemini
 */
export async function getAICompletion(
  systemPrompt: string,
  userPrompt: string,
  options: {
    temperature?: number;
    maxTokens?: number;
  } = {}
): Promise<AIResponse> {
  const { temperature = 0.7, maxTokens = 4000 } = options;

  // Try Groq first (faster and better for chat)
  const groqKey = process.env.GROQ_API_KEY;
  if (groqKey) {
    try {
      const content = await callGroq(systemPrompt, userPrompt, groqKey, temperature, maxTokens);
      return { content, provider: 'groq' };
    } catch (error: any) {
      console.error('Groq AI error, falling back to Gemini:', error.message);
    }
  }

  // Fallback to Gemini
  const geminiKey = process.env.GEMINI_API_KEY;
  if (!geminiKey) {
    throw new Error('No AI provider configured. Set GROQ_API_KEY or GEMINI_API_KEY');
  }

  try {
    const content = await callGemini(systemPrompt, userPrompt, geminiKey, temperature, maxTokens);
    return { content, provider: 'gemini' };
  } catch (error: any) {
    // Only log non-rate-limit errors
    if (!error.message?.includes('rate limit')) {
      console.error('Gemini AI error:', error);
    }
    throw error;
  }
}

/**
 * Call Groq API (ULTRA FAST!)
 * Using llama-3.3-70b-versatile
 */
async function callGroq(
  systemPrompt: string,
  userPrompt: string,
  apiKey: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: 'llama-3.3-70b-versatile',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature,
      max_tokens: maxTokens,
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Groq API error: ${response.statusText} - ${error}`);
  }

  const data = await response.json();
  
  if (data.error) {
    throw new Error(`Groq API error: ${data.error.message || JSON.stringify(data.error)}`);
  }
  
  const text = data.choices?.[0]?.message?.content;
  if (text) {
    return text;
  }
  
  throw new Error('No text in Groq response');
}

/**
 * Call Gemini API (FREE and FAST!)
 */
async function callGemini(
  systemPrompt: string,
  userPrompt: string,
  apiKey: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  // Use latest stable Gemini model
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: `${systemPrompt}\n\n${userPrompt}`
          }]
        }],
        generationConfig: {
          temperature,
          maxOutputTokens: maxTokens,
        }
      }),
    }
  );

  if (!response.ok) {
    const error = await response.text();
    // Check if it's a rate limit error
    if (response.status === 429) {
      throw new Error('Gemini API rate limit exceeded. Using local analysis.');
    }
    throw new Error(`Gemini API error: ${response.statusText}`);
  }

  const data = await response.json();
  
  // Check for error in response
  if (data.error) {
    throw new Error(`Gemini API error: ${data.error.message || JSON.stringify(data.error)}`);
  }
  
  // Check if response was cut off due to token limit
  const candidate = data.candidates?.[0];
  if (candidate?.finishReason === 'MAX_TOKENS') {
    // Return partial response instead of throwing
    const partialText = candidate?.content?.parts?.[0]?.text;
    if (partialText) {
      console.warn('Gemini response truncated, returning partial response');
      return partialText + '\n\n[Response truncated due to length]';
    }
    throw new Error('Gemini response truncated (max tokens reached). Increase maxOutputTokens.');
  }
  
  // Extract text from response
  const text = candidate?.content?.parts?.[0]?.text;
  if (text) {
    return text;
  }
  
  // If no text, throw error with details
  throw new Error(`No text in Gemini response. Finish reason: ${candidate?.finishReason || 'unknown'}`);
}
